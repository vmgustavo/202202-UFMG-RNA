{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "import warnings\n",
    "from time import time\n",
    "from functools import reduce\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from typing import Tuple\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn import datasets\n",
    "import requests_cache\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def url_request(url: str, name: str) -> BytesIO:\n",
    "    session = requests_cache.CachedSession(\n",
    "        cache_name=name,\n",
    "        expire_after=timedelta(days=1)\n",
    "    )\n",
    "    response = session.get(url)\n",
    "\n",
    "    memfile = BytesIO()\n",
    "    memfile.write(response.content)\n",
    "    memfile.seek(0)\n",
    "\n",
    "    return memfile\n",
    "\n",
    "\n",
    "def scaler(dataset: callable, *args, **kwargs):  # noqa\n",
    "    def wrapper(*args, **kwargs) -> Tuple[pd.DataFrame, pd.Series]:  # noqa\n",
    "        data, target = dataset(*args, **kwargs)\n",
    "        scaler_ = StandardScaler()\n",
    "        data_scaled = pd.DataFrame(\n",
    "            data=scaler_.fit_transform(data),\n",
    "            columns=data.columns\n",
    "        )\n",
    "        return data_scaled, target\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_linear(n_obs: int = 2048, n_feats: int = 2) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    data, target = datasets.make_blobs(\n",
    "        n_samples=n_obs, n_features=n_feats,\n",
    "        centers=[[-2, -2], [2, 2]],\n",
    "        cluster_std=1.5,\n",
    "    )\n",
    "    target = (target == 1) * 2 - 1\n",
    "    return pd.DataFrame(data), pd.Series(target)\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_blobs(n_obs: int = 2048, n_feats: int = 2) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    data, target = datasets.make_blobs(\n",
    "        n_samples=n_obs, n_features=n_feats,\n",
    "        centers=[\n",
    "            [-2, -2], [2, 2],\n",
    "            [-2, 2], [2, -2],\n",
    "        ],\n",
    "        cluster_std=1.4,\n",
    "    )\n",
    "    target = np.isin(target, [0, 1]) * 2 - 1\n",
    "    return pd.DataFrame(data), pd.Series(target)\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_moons(n_obs: int = 2048) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    data, target = datasets.make_moons(\n",
    "        n_samples=n_obs, noise=0.2,\n",
    "    )\n",
    "\n",
    "    target = (target == 1) * 2 - 1\n",
    "    data = data - [data[:, 0].mean(), data[:, 1].mean()]\n",
    "    return pd.DataFrame(data), pd.Series(target)\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_ilpd() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\" ILPD (Indian Liver Patient Dataset) Data Set\n",
    "    Data Set Information: This data set contains 416 liver patient records and\n",
    "    167 non liver patient records. The data set was collected from north east of\n",
    "    Andhra Pradesh, India. Selector is a class label used to divide into\n",
    "    groups (liver patient or not). This data set contains 441 male patient\n",
    "    records and 142 female patient records.\n",
    "\n",
    "    Any patient whose age exceeded 89 is listed as being of age \"90\".\n",
    "\n",
    "    Attribute Information:\n",
    "        1. Age: Age of the patient\n",
    "        2. Gender: Gender of the patient\n",
    "        3. TB: Total Bilirubin\n",
    "        4. DB: Direct Bilirubin\n",
    "        5. Alkphos: Alkaline Phosphotase\n",
    "        6. Sgpt: Alamine Aminotransferase\n",
    "        7. Sgot: Aspartate Aminotransferase\n",
    "        8. TP: Total Protiens\n",
    "        9. ALB: Albumin\n",
    "        10. A/G: Ratio Albumin and Globulin Ratio\n",
    "        11. Selector field used to split the data into two sets (labeled by the experts)\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)\n",
    "    \"\"\"\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/00225\"\n",
    "            + \"/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"ilpd\"), header=None)\n",
    "    target = df.iloc[:, -1] * 2 - 3\n",
    "\n",
    "    data = df.iloc[:, 0:-1]\n",
    "    data.columns = [\"age\", \"gender\", \"tb\", \"db\", \"alkphos\", \"sgpt\", \"sgot\", \"tp\", \"alb\", \"ag\"]\n",
    "    data[\"gender\"] = (data[\"gender\"] == \"Female\").astype(\"int\")\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_australian_credit() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Statlog (Australian Credit Approval) Data Set\n",
    "\n",
    "    Data Set Information: This file concerns credit card applications. All\n",
    "    attribute names and values have been changed to meaningless symbols to\n",
    "    protect confidentiality of the data.\n",
    "\n",
    "    This dataset is interesting because there is a good mix of\n",
    "    attributes -- continuous, nominal with small numbers of values, and nominal\n",
    "    with larger numbers of values. There are also a few missing values.\n",
    "\n",
    "    Attribute Information: There are 6 numerical and 8 categorical attributes.\n",
    "    The labels have been changed for the convenience of the statistical\n",
    "    algorithms. For example, attribute 4 originally had 3 labels p,g,gg and\n",
    "    these have been changed to labels 1,2,3.\n",
    "\n",
    "    A1: 0,1 CATEGORICAL (formerly: a,b)\n",
    "    A2: continuous.\n",
    "    A3: continuous.\n",
    "    A4: 1,2,3 CATEGORICAL (formerly: p,g,gg)\n",
    "    A5: 1,2,3,4,5,6,7,8,9,10,11,12,13,14 CATEGORICAL (formerly: ff,d,i,k,j,aa,m,c,w,e,q,r,cc,x)\n",
    "    A6: 1,2,3,4,5,6,7,8,9 CATEGORICAL (formerly: ff,dd,j,bb,v,n,o,h,z)\n",
    "    A7: continuous.\n",
    "    A8: 1, 0 CATEGORICAL (formerly: t, f)\n",
    "    A9: 1, 0 CATEGORICAL (formerly: t, f)\n",
    "    A10: continuous.\n",
    "    A11: 1, 0 CATEGORICAL (formerly t, f)\n",
    "    A12: 1, 2, 3 CATEGORICAL (formerly: s, g, p)\n",
    "    A13: continuous.\n",
    "    A14: continuous.\n",
    "    A15: 1,2 class attribute (formerly: +,-)\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/statlog\"\n",
    "            + \"/australian/australian.dat\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"aus_credit\"), header=None, sep=\" \")\n",
    "    target = df.iloc[:, -1] * 2 - 1\n",
    "\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [f\"A{i}\" for i in range(1, data.shape[1] + 1)]\n",
    "    data = data.astype({\n",
    "        \"A1\": \"category\",\n",
    "        \"A2\": \"float\",\n",
    "        \"A3\": \"float\",\n",
    "        \"A4\": \"category\",\n",
    "        \"A5\": \"category\",\n",
    "        \"A6\": \"category\",\n",
    "        \"A7\": \"float\",\n",
    "        \"A8\": \"category\",\n",
    "        \"A9\": \"category\",\n",
    "        \"A10\": \"int\",\n",
    "        \"A11\": \"category\",\n",
    "        \"A12\": \"category\",\n",
    "        \"A13\": \"int\",\n",
    "        \"A14\": \"int\",\n",
    "    })\n",
    "\n",
    "    cats = data.select_dtypes(\"category\")\n",
    "    conts = data[[col for col in data.columns if col not in cats.columns]]\n",
    "\n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse=False)\n",
    "    data = pd.concat(\n",
    "        [conts,\n",
    "         pd.DataFrame(encoder.fit_transform(cats)).astype(\"int\")],\n",
    "        axis=1\n",
    "    )\n",
    "    data.columns = map(str, data.columns)\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_banknote() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data were extracted from images that were taken from genuine and forged\n",
    "    banknote-like specimens. For digitization, an industrial camera usually used\n",
    "    for print inspection was used. The final images have 400x 400 pixels. Due to\n",
    "    the object lens and distance to the investigated object gray-scale pictures\n",
    "    with a resolution of about 660 dpi were gained. Wavelet Transform tool were\n",
    "    used to extract features from images.\n",
    "\n",
    "    Attribute Information:\n",
    "    1. variance of Wavelet Transformed image (continuous)\n",
    "    2. skewness of Wavelet Transformed image (continuous)\n",
    "    3. curtosis of Wavelet Transformed image (continuous)\n",
    "    4. entropy of image (continuous)\n",
    "    5. class (integer)\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/00267\"\n",
    "            + \"/data_banknote_authentication.txt\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"banknote\"), header=None, sep=\",\")\n",
    "    target = df.iloc[:, -1] * 2 - 1\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [\"var\", \"skew\", \"kurt\", \"entropy\"]\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_breast_cancer_coimbra() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data Set Information: There are 10 predictors, all quantitative, and a\n",
    "    binary dependent variable, indicating the presence or absence of breast\n",
    "    cancer. The predictors are anthropometric data and parameters which can be\n",
    "    gathered in routine blood analysis. Prediction models based on these\n",
    "    predictors, if accurate, can potentially be used as a biomarker of breast\n",
    "    cancer.\n",
    "\n",
    "    Attribute Information:\n",
    "        Quantitative Attributes:\n",
    "        Age (years)\n",
    "        BMI (kg/m2)\n",
    "        Glucose (mg/dL)\n",
    "        Insulin (µU/mL)\n",
    "        HOMA\n",
    "        Leptin (ng/mL)\n",
    "        Adiponectin (µg/mL)\n",
    "        Resistin (ng/mL)\n",
    "        MCP-1(pg/dL)\n",
    "\n",
    "        Labels:\n",
    "        1=Healthy controls\n",
    "        2=Patients\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/00451\"\n",
    "            + \"/dataR2.csv\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"breast_coimbra\"), header=0)\n",
    "    target = df.iloc[:, -1] * 2 - 3\n",
    "    data = df.iloc[:, :-1]\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_breast_cancer_wisconsin() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data Set Information: Features are computed from a digitized image of a\n",
    "    fine needle aspirate (FNA) of a breast mass. They describe characteristics\n",
    "    of the cell nuclei present in the image. Separating plane described above\n",
    "    was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett,\n",
    "    \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "    Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101,\n",
    "    1992], a classification method which uses linear programming to construct a\n",
    "    decision tree. Relevant features were selected using an exhaustive search in\n",
    "    the space of 1-4 features and 1-3 separating planes.\n",
    "\n",
    "    The actual linear program used to obtain the separating plane in the\n",
    "    3-dimensional space is that described in: [K. P. Bennett and\n",
    "    O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly\n",
    "    Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "    #  Attribute                     Domain\n",
    "    -- -----------------------------------------\n",
    "    1. Sample code number            id number\n",
    "    2. Clump Thickness               1 - 10\n",
    "    3. Uniformity of Cell Size       1 - 10\n",
    "    4. Uniformity of Cell Shape      1 - 10\n",
    "    5. Marginal Adhesion             1 - 10\n",
    "    6. Single Epithelial Cell Size   1 - 10\n",
    "    7. Bare Nuclei                   1 - 10\n",
    "    8. Bland Chromatin               1 - 10\n",
    "    9. Normal Nucleoli               1 - 10\n",
    "    10. Mitoses                      1 - 10\n",
    "    11. Class:                       (2 for benign, 4 for malignant)\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/breast-cancer-wisconsin\"\n",
    "            + \"/breast-cancer-wisconsin.data\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"breast_wisconsin\"), header=None)\n",
    "    df = (\n",
    "        df\n",
    "        .replace(\"?\", np.nan)\n",
    "        .astype(\"float\")\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    target = df.iloc[:, -1] - 3\n",
    "    data = df.iloc[:, 1:-1]\n",
    "    data.columns = [\n",
    "        \"clump_thickness\", \"uniformity_cell_size\", \"uniformity_cell_shape\",\n",
    "        \"marginal_adhesion\", \"single_epith_cell_size\", \"bare_nuclei\",\n",
    "        \"bland_chromatin\", \"normal_nucleioli\", \"mitoses\",\n",
    "    ]\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_german_credit() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data Set Information: Two datasets are provided. the original dataset,\n",
    "    in the form provided by Prof. Hofmann, contains categorical/symbolic\n",
    "    attributes and is in the file \"german.data\".\n",
    "\n",
    "    For algorithms that need numerical attributes, Strathclyde University\n",
    "    produced the file \"german.data-numeric\". This file has been edited and\n",
    "    several indicator variables added to make it suitable for algorithms which\n",
    "    cannot cope with categorical variables. Several attributes that are ordered\n",
    "    categorical (such as attribute 17) have been coded as integer. This was the\n",
    "    form used by StatLog.\n",
    "\n",
    "    It is worse to class a customer as good when they are bad, than it is to\n",
    "    class a customer as bad when they are good.\n",
    "\n",
    "    Attribute Information:\n",
    "\n",
    "        Attribute 1: (qualitative)\n",
    "        Status of existing checking account\n",
    "        A11 : ... < 0 DM\n",
    "        A12 : 0 <= ... < 200 DM\n",
    "        A13 : ... >= 200 DM / salary assignments for at least 1 year\n",
    "        A14 : no checking account\n",
    "\n",
    "        Attribute 2: (numerical)\n",
    "        Duration in month\n",
    "\n",
    "        Attribute 3: (qualitative)\n",
    "        Credit history\n",
    "        A30 : no credits taken/ all credits paid back duly\n",
    "        A31 : all credits at this bank paid back duly\n",
    "        A32 : existing credits paid back duly till now\n",
    "        A33 : delay in paying off in the past\n",
    "        A34 : critical account/ other credits existing (not at this bank)\n",
    "\n",
    "        Attribute 4: (qualitative)\n",
    "        Purpose\n",
    "        A40 : car (new)\n",
    "        A41 : car (used)\n",
    "        A42 : furniture/equipment\n",
    "        A43 : radio/television\n",
    "        A44 : domestic appliances\n",
    "        A45 : repairs\n",
    "        A46 : education\n",
    "        A47 : (vacation - does not exist?)\n",
    "        A48 : retraining\n",
    "        A49 : business\n",
    "        A410 : others\n",
    "\n",
    "        Attribute 5: (numerical)\n",
    "        Credit amount\n",
    "\n",
    "        Attibute 6: (qualitative)\n",
    "        Savings account/bonds\n",
    "        A61 : ... < 100 DM\n",
    "        A62 : 100 <= ... < 500 DM\n",
    "        A63 : 500 <= ... < 1000 DM\n",
    "        A64 : .. >= 1000 DM\n",
    "        A65 : unknown/ no savings account\n",
    "\n",
    "        Attribute 7: (qualitative)\n",
    "        Present employment since\n",
    "        A71 : unemployed\n",
    "        A72 : ... < 1 year\n",
    "        A73 : 1 <= ... < 4 years\n",
    "        A74 : 4 <= ... < 7 years\n",
    "        A75 : .. >= 7 years\n",
    "\n",
    "        Attribute 8: (numerical)\n",
    "        Installment rate in percentage of disposable income\n",
    "\n",
    "        Attribute 9: (qualitative)\n",
    "        Personal status and sex\n",
    "        A91 : male : divorced/separated\n",
    "        A92 : female : divorced/separated/married\n",
    "        A93 : male : single\n",
    "        A94 : male : married/widowed\n",
    "        A95 : female : single\n",
    "\n",
    "        Attribute 10: (qualitative)\n",
    "        Other debtors / guarantors\n",
    "        A101 : none\n",
    "        A102 : co-applicant\n",
    "        A103 : guarantor\n",
    "\n",
    "        Attribute 11: (numerical)\n",
    "        Present residence since\n",
    "\n",
    "        Attribute 12: (qualitative)\n",
    "        Property\n",
    "        A121 : real estate\n",
    "        A122 : if not A121 : building society savings agreement/ life insurance\n",
    "        A123 : if not A121/A122 : car or other, not in attribute 6\n",
    "        A124 : unknown / no property\n",
    "\n",
    "        Attribute 13: (numerical)\n",
    "        Age in years\n",
    "\n",
    "        Attribute 14: (qualitative)\n",
    "        Other installment plans\n",
    "        A141 : bank\n",
    "        A142 : stores\n",
    "        A143 : none\n",
    "\n",
    "        Attribute 15: (qualitative)\n",
    "        Housing\n",
    "        A151 : rent\n",
    "        A152 : own\n",
    "        A153 : for free\n",
    "\n",
    "        Attribute 16: (numerical)\n",
    "        Number of existing credits at this bank\n",
    "\n",
    "        Attribute 17: (qualitative)\n",
    "        Job\n",
    "        A171 : unemployed/ unskilled - non-resident\n",
    "        A172 : unskilled - resident\n",
    "        A173 : skilled employee / official\n",
    "        A174 : management/ self-employed/\n",
    "        highly qualified employee/ officer\n",
    "\n",
    "        Attribute 18: (numerical)\n",
    "        Number of people being liable to provide maintenance for\n",
    "\n",
    "        Attribute 19: (qualitative)\n",
    "        Telephone\n",
    "        A191 : none\n",
    "        A192 : yes, registered under the customers name\n",
    "\n",
    "        Attribute 20: (qualitative)\n",
    "        foreign worker\n",
    "        A201 : yes\n",
    "        A202 : no\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/statlog\"\n",
    "            + \"/german/german.data\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"german_credit\"), header=None, sep=\" \")\n",
    "    target = df.iloc[:, -1] * 2 - 3\n",
    "\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [f\"A{i}\" for i in range(1, data.shape[1] + 1)]\n",
    "\n",
    "    cats = data.select_dtypes(\"object\")\n",
    "    conts = data[[col for col in data.columns if col not in cats.columns]]\n",
    "\n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse=False)\n",
    "    data = pd.concat(\n",
    "        [conts,\n",
    "         pd.DataFrame(encoder.fit_transform(cats)).astype(\"int\")],\n",
    "        axis=1\n",
    "    )\n",
    "    data.columns = map(str, data.columns)\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_haberman_survival() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data Set Information: The dataset contains cases from a study that was\n",
    "    conducted between 1958 and 1970 at the University of Chicago's Billings\n",
    "    Hospital on the survival of patients who had undergone surgery for breast\n",
    "    cancer.\n",
    "\n",
    "    Attribute Information:\n",
    "        1. Age of patient at time of operation (numerical)\n",
    "        2. Patient's year of operation (year - 1900, numerical)\n",
    "        3. Number of positive axillary nodes detected (numerical)\n",
    "        4. Survival status (class attribute)\n",
    "        -- 1 = the patient survived 5 years or longer\n",
    "        -- 2 = the patient died within 5 year\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/haberman%27s+survival\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/haberman\"\n",
    "            + \"/haberman.data\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"haberman_survival\"), header=None, sep=\",\")\n",
    "    target = df.iloc[:, -1] * 2 - 3\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [\"age\", \"operation_year\", \"n_nodes\"]\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_sonar() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Data Set Information: The file \"sonar.mines\" contains 111 patterns\n",
    "    obtained by bouncing sonar signals off a metal cylinder at various angles\n",
    "    and under various conditions. The file \"sonar.rocks\" contains 97 patterns\n",
    "    obtained from rocks under similar conditions. The transmitted sonar signal\n",
    "    is a frequency-modulated chirp, rising in frequency. The data set contains\n",
    "    signals obtained from a variety of different aspect angles, spanning 90\n",
    "    degrees for the cylinder and 180 degrees for the rock.\n",
    "\n",
    "    Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number\n",
    "    represents the energy within a particular frequency band, integrated over a\n",
    "    certain period of time. The integration aperture for higher frequencies\n",
    "    occur later in time, since these frequencies are transmitted later during\n",
    "    the chirp.\n",
    "\n",
    "    The label associated with each record contains the letter \"R\" if the object\n",
    "    is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the\n",
    "    labels are in increasing order of aspect angle, but they do not encode the\n",
    "    angle directly.\n",
    "\n",
    "    Source: http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"http://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/undocumented/connectionist-bench\"\n",
    "            + \"/sonar/sonar.all-data\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"sonar\"), header=None, sep=\",\")\n",
    "    target = (df.iloc[:, -1] == \"M\").astype(\"int\") * 2 - 1\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [f\"A{i}\" for i in range(data.shape[1])]\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "@scaler\n",
    "def get_heart() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"This dataset is a heart disease database similar to a database already\n",
    "    present in the repository (Heart Disease databases) but in a slightly\n",
    "    different form\n",
    "\n",
    "    Attribute Information:\n",
    "    ------------------------\n",
    "    -- 1. age\n",
    "    -- 2. sex\n",
    "    -- 3. chest pain type (4 values)\n",
    "    -- 4. resting blood pressure\n",
    "    -- 5. serum cholesterol in mg/dl\n",
    "    -- 6. fasting blood sugar > 120 mg/dl\n",
    "    -- 7. resting electrocardiographic results (values 0,1,2)\n",
    "    -- 8. maximum heart rate achieved\n",
    "    -- 9. exercise induced angina\n",
    "    -- 10. oldpeak = ST depression induced by exercise relative to rest\n",
    "    -- 11. the slope of the peak exercise ST segment\n",
    "    -- 12. number of major vessels (0-3) colored by flourosopy\n",
    "    -- 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "    Variable to be predicted\n",
    "    ------------------------\n",
    "    Absence (1) or presence (2) of heart disease\n",
    "\n",
    "    Source: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "            \"https://archive.ics.uci.edu\"\n",
    "            + \"/ml/machine-learning-databases/statlog\"\n",
    "            + \"/heart/heart.dat\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(url_request(url, \"heart\"), header=None, sep=\" \")\n",
    "    target = df.iloc[:, -1] * 2 - 3\n",
    "    data = df.iloc[:, :-1]\n",
    "    data.columns = [\n",
    "        \"age\", \"sex\", \"chest_pain_type\", \"rest_blood_pressure\", \"serum_cholesterol\",\n",
    "        \"fasting_blood_sugar\", \"resting_ekg\", \"heart_rate\", \"exercise_induced_angina\",\n",
    "        \"oldpeak\", \"slope_peak\", \"n_major_vessels\", \"thal\",\n",
    "    ]\n",
    "\n",
    "    cats = data[[\n",
    "        \"sex\", \"chest_pain_type\", \"fasting_blood_sugar\", \"resting_ekg\",\n",
    "        \"exercise_induced_angina\", \"thal\",\n",
    "    ]]\n",
    "    conts = data[[col for col in data.columns if col not in cats.columns]]\n",
    "\n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse=False)\n",
    "    data = pd.concat(\n",
    "        [conts,\n",
    "         pd.DataFrame(encoder.fit_transform(cats)).astype(\"int\")],\n",
    "        axis=1\n",
    "    )\n",
    "    data.columns = map(str, data.columns)\n",
    "\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def alldts() -> dict:\n",
    "    return {\n",
    "        \"synth_linear\": get_linear(),\n",
    "        \"synth_blobs\": get_blobs(),\n",
    "        \"synth_moons\": get_moons(),\n",
    "        \"cred_aus\": get_australian_credit(),\n",
    "        \"cred_ger\": get_german_credit(),\n",
    "        \"banknote\": get_banknote(),\n",
    "        \"breast_coimbra\": get_breast_cancer_coimbra(),\n",
    "        \"breast_wiscons\": get_breast_cancer_wisconsin(),\n",
    "        \"haberman_surv\": get_haberman_survival(),\n",
    "        \"sonar\": get_sonar(),\n",
    "        \"heart\": get_heart(),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "class GabrielGraph:\n",
    "    adj_mat_: coo_matrix\n",
    "    distmat_: np.array\n",
    "\n",
    "    def __init__(self, X: pd.DataFrame) -> None:\n",
    "        self.X = X\n",
    "\n",
    "    def adjacency(self) -> coo_matrix:\n",
    "        n_obs = self.X.shape[0]\n",
    "        self.distmat_ = np.power(distance.squareform(distance.pdist(self.X)), 2)\n",
    "        self.distmat_[np.diag_indices(n_obs)] = np.inf\n",
    "\n",
    "        with Pool() as pool:\n",
    "            params = product(range(n_obs), range(n_obs))\n",
    "            res = pool.starmap(self.calc, params)\n",
    "        res = np.array(list(filter(lambda x: x is not None, res)))\n",
    "\n",
    "        n_adjs = len(res)\n",
    "        adj_mat = coo_matrix((np.ones(n_adjs), (res[:, 0], res[:, 1])), shape=(n_obs, n_obs))\n",
    "\n",
    "        self.adj_mat_ = adj_mat\n",
    "        return adj_mat\n",
    "\n",
    "    def calc(self, i: int, j: int):\n",
    "        minimum = np.min(self.distmat_[i, :] + self.distmat_[j, :])\n",
    "        if self.distmat_[i, j] <= minimum:\n",
    "            return [i, j]\n",
    "\n",
    "    def plot(self):\n",
    "        graph = nx.from_scipy_sparse_array(self.adj_mat_)\n",
    "        pos = {i: tuple(elem) for i, elem in self.X.iterrows()}\n",
    "\n",
    "        options = {\n",
    "            \"font_size\": 1,\n",
    "            \"node_size\": 50,\n",
    "            \"node_color\": \"white\",\n",
    "            \"edgecolors\": \"black\",\n",
    "            \"linewidths\": 1,\n",
    "            \"width\": 1,\n",
    "        }\n",
    "        nx.draw_networkx(graph, pos, **options)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.margins(0.20)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Topology:\n",
    "    def __init__(self, data: pd.DataFrame, target: pd.Series, adjacency: coo_matrix):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.adjacency = adjacency\n",
    "\n",
    "    @staticmethod\n",
    "    def get_quality(curr_obs: int, adjacency: coo_matrix, target_: pd.Series):\n",
    "        class_links = target_[np.where((adjacency.getcol(curr_obs) > 0).toarray())[0]]\n",
    "        return pd.DataFrame(class_links == target_[curr_obs]).astype(\"int\").mean().iloc[0]\n",
    "\n",
    "    def executor(self, x):\n",
    "        return self.get_quality(x, self.adjacency, self.target)\n",
    "\n",
    "    def class_quality(self):\n",
    "        with Pool() as pool:\n",
    "            results = pool.map(self.executor, range(self.data.shape[0]))\n",
    "\n",
    "        class_quality = pd.DataFrame([self.target, pd.Series(results)]).transpose()\n",
    "        class_quality.columns = [\"target\", \"link_prop\"]\n",
    "\n",
    "        def quality(x: float):\n",
    "            if x == 0:\n",
    "                return \"isolated\"\n",
    "            elif x == 1:\n",
    "                return \"normal\"\n",
    "            else:\n",
    "                return \"border\"\n",
    "\n",
    "        class_quality[\"quality\"] = class_quality[\"link_prop\"].apply(quality)\n",
    "        return class_quality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    silhouette_samples, silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")\n",
    "\n",
    "\n",
    "def sil_neg_samples_score(X, labels):\n",
    "    res = silhouette_samples(X, labels)\n",
    "\n",
    "    counts = Counter(res > 0)\n",
    "    return counts[False] / (counts[False] + counts[True])\n",
    "\n",
    "\n",
    "class GGMetrics:\n",
    "    def __init__(self, X, labels):\n",
    "        gg = GabrielGraph(X)\n",
    "        adj_mat = gg.adjacency()\n",
    "        tpl = Topology(X, labels, adj_mat)\n",
    "        self.quality = tpl.class_quality()\n",
    "        self.scores = np.array(self.quality[\"link_prop\"].values)\n",
    "\n",
    "    def gg_neigh_index(self, *args, **kwargs):  # noqa\n",
    "        return np.mean(self.scores)\n",
    "\n",
    "    def gg_border_perc(self, *args, **kwargs):  # noqa\n",
    "        test = (self.quality[\"quality\"] == \"border\").values\n",
    "        counts = Counter(test)\n",
    "        return counts[True] / test.shape[0]\n",
    "\n",
    "    def gg_class_quality(self):\n",
    "        return (\n",
    "            self.quality\n",
    "            .groupby([\"target\", \"quality\"])\n",
    "            .agg({\"quality\": \"count\"})\n",
    "            .rename(columns={\"quality\": \"count\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "\n",
    "def cluster_evaluate(X, labels):\n",
    "    gg_metrics = GGMetrics(X, labels)\n",
    "\n",
    "    metrics = [\n",
    "        gg_metrics.gg_neigh_index,\n",
    "        gg_metrics.gg_border_perc,\n",
    "        silhouette_score,\n",
    "        sil_neg_samples_score,\n",
    "        calinski_harabasz_score,\n",
    "        davies_bouldin_score,\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        metric.__name__: metric(X=X, labels=labels)\n",
    "        for metric in metrics\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def repeat(n):\n",
    "    for _ in range(n):\n",
    "        executor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelResults:\n",
    "    X_train: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_test: pd.Series\n",
    "    model: MLPClassifier\n",
    "    hidden_layer_sizes: tuple\n",
    "    reg_alpha: float\n",
    "    projection: np.array\n",
    "    pred_train: np.array\n",
    "    pred_test: np.array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exec_nn(data: pd.DataFrame, target: pd.Series, hidden_layer_sizes: tuple, reg_alpha: float):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.values, target.values,\n",
    "        stratify=target, test_size=.3\n",
    "    )\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes, activation=\"tanh\", solver=\"adam\",\n",
    "        alpha=reg_alpha, beta_1=0.9, beta_2=0.999,\n",
    "        max_iter=256,\n",
    "        verbose=False, shuffle=False,\n",
    "        early_stopping=False, validation_fraction=0.1,\n",
    "        n_iter_no_change=512, tol=1e-6,\n",
    "        epsilon=1e-8, learning_rate=\"constant\",\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    def feed_forward(a, b):\n",
    "        a = np.hstack([np.ones((a.shape[0], 1)), a])\n",
    "        return np.tanh(a @ b)\n",
    "\n",
    "    coefs = [np.vstack([a.reshape(1, -1), b]) for a, b in zip(model.intercepts_[:-1], model.coefs_[:-1])]\n",
    "    projection = reduce(feed_forward, [X_train] + coefs)\n",
    "\n",
    "    return ModelResults(\n",
    "        X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,\n",
    "        model=model,\n",
    "        hidden_layer_sizes=hidden_layer_sizes, reg_alpha=reg_alpha,\n",
    "        projection=projection,\n",
    "        pred_train=model.predict(X_train), pred_test=model.predict(X_test),\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def executor():\n",
    "    selection = {\n",
    "        \"cred_aus\", \"cred_ger\",\n",
    "        \"breast_coimbra\", \"sonar\", \"heart\",\n",
    "        \"synth_linear\", \"synth_blobs\", \"synth_moons\",\n",
    "    }\n",
    "    dts = [(k, v) for k, v in alldts().items() if k in selection]\n",
    "    alphas = np.logspace(0, 1, 16)\n",
    "\n",
    "    for dataset_name, (data, target) in dts:\n",
    "        for alpha in alphas:\n",
    "            logging.info(f\"dataset: {dataset_name:>15} | alpha: {alpha:02.02f}\")\n",
    "            res = exec_nn(\n",
    "                data, target,\n",
    "                hidden_layer_sizes=(128, 128, 128, 128, 128), reg_alpha=alpha,\n",
    "            )\n",
    "\n",
    "            results = dict(\n",
    "                {\n",
    "                    \"alpha\": alpha,\n",
    "                    \"acc_train\": accuracy_score(y_pred=res.pred_train, y_true=res.y_train),\n",
    "                    \"acc_test\": accuracy_score(y_pred=res.pred_test, y_true=res.y_test),\n",
    "                    \"best_loss\": res.model.best_loss_,\n",
    "                    \"iterations\": res.model.n_iter_\n",
    "                },\n",
    "                **cluster_evaluate(X=res.projection, labels=res.y_train),\n",
    "                **{f\"orig_{k}\": v for k, v in cluster_evaluate(X=res.X_train, labels=res.y_train).items()}\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(\"results\"):\n",
    "                os.mkdir(\"results\")\n",
    "\n",
    "            fname = str(time()) + str(alpha) + dataset_name\n",
    "            with open(f\"results/{hashlib.sha256(fname.encode()).hexdigest()}.json\", \"w\") as f:\n",
    "                json.dump(results, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    executor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
