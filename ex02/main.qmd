---
title: "Redes Neurais Artificiais"
subtitle: Exercício de Regularização de ELMs
author: Gustavo Vieira Maia
editor:
    render-on-save: true
lang: pt-br
format:
    html:
        toc: true
        number-sections: true
        color-links: true
        geometry:
            - left=30mm
            - right=30mm
---

<div id="toc"></div>

\newpage

# Objetivo

Implementação de uma rede neural de duas camadas com capacidade para a resolução de problemas não-lineares e estudar os efeitos da regularização sobre o desempenho do modelo.

## Bibliotecas relevantes para o código

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
```

# Definição de Extreme Learning Machines (ELM)

$$
X_{(N, n)} \xrightarrow{Z_{n, p}} H_{(n, p)} \xrightarrow{w_{(p, 1)}} y_{(N, 1)}
$$

## Teste ELM

### Especificação de parâmetros

```{python}
np.random.seed(0)

std_dev = 1.2
neurons_hidden = 10
neurons_output = 1
n_samples = 50
reg = 0
```

### Geração de classes para problemas de separação não-lineares

```{python}
class_0 = np.vstack([
    (
        np.random.normal(size=(n_samples, 2), scale=std_dev)
        + np.tile([2, 2], n_samples).reshape(-1, 2)
    ),
    (
        np.random.normal(size=(n_samples, 2), scale=std_dev)
        - np.tile([2, 2], n_samples).reshape(-1, 2)
    )
])

class_1 = np.vstack([
    (
        np.random.normal(size=(n_samples, 2), scale=std_dev)
        + np.tile([2, -2], n_samples).reshape(-1, 2)
    ),
    (
        np.random.normal(size=(n_samples, 2), scale=std_dev)
        + np.tile([-2, 2], n_samples).reshape(-1, 2)
    )
])

data = np.vstack([class_0, class_1])
target = np.repeat([-1, 1], repeats=n_samples * 2).reshape(-1, 1)
data.shape, target.shape
```

```{python}
plt.figure()
plt.scatter(data[:, 0], data[:, 1], c=target)
plt.show()
plt.close()
```

**Geração de pesos aleatórios para dados de entrada**

```{python}
input_weights = np.random.uniform(size=(data.shape[1] + 1, neurons_hidden))
input_weights.shape
```

**Transformação com função de ativação não linear**

```{python}
hidden_input = np.dot(
    np.hstack([np.ones(shape=(data.shape[0], 1)), data]),
    input_weights
)
hidden_input.shape
```

```{python}
hidden_output = np.tanh(hidden_input)
hidden_output = np.hstack([np.ones(shape=(data.shape[0], 1)), hidden_output])
hidden_output.shape
```

**Cálculo de pesos da camada de saída**

```{python}
output_weights = np.dot(
    np.linalg.pinv(hidden_output),
    target
)
output_weights.shape
```

Adição de regularização

```{python}
aux0 = np.linalg.pinv(
    np.dot(hidden_output.T, hidden_output)
    + (reg * np.identity(neurons_hidden + 1))
)
aux1 = np.dot(np.linalg.pinv(aux0), hidden_output.T)
output_weights_reg = np.dot(aux1, target)
```

**Cálculo dos valores de saída da rede**

```{python}
output_input = np.dot(
    hidden_output,
    output_weights
)
output_input.shape
```

```{python}
output_output = output_input * 1
```

**Avaliação de resultados da classificação**

```{python}
print(classification_report(output_output > 0, target > 0))
```

**Cálculo dos valores de saída da rede com regularização**

```{python}
output_input = np.dot(
    hidden_output,
    output_weights_reg
)
output_input.shape
```

```{python}
output_output = output_input * 1
```

**Avaliação de resultados da classificação com regularização**

```{python}
print(classification_report(output_output > 0, target > 0))
```

## Modelo Classificador ELM

```{python}
class ELMClassifier:
    def __init__(self, neurons: int, seed: int = 0):
        self.neurons = neurons
        self.seed = seed

    def fit(self, X, y):
        # GENERATE RANDOM HIDDEN LAYER WEIGHTS
        self.w_input_ = np.random.uniform(size=(X.shape[1] + 1, self.neurons))

        # CALCULATE HIDDEN LAYER INPUT
        hidden_input = np.dot(
            np.hstack([np.ones(shape=(X.shape[0], 1)), X]),
            self.w_input_
        )

        # CALCULATE HIDDEN LAYER OUTPUT
        hidden_output = np.tanh(hidden_input)
        hidden_output = np.hstack([np.ones(shape=(X.shape[0], 1)), hidden_output])
        
        # CALCULATE OUTPUT LAYER WEIGHTS
        self.w_output_ = np.dot(np.linalg.pinv(hidden_output), y)

        return self

    def predict(self, X):
        # CALCULATE HIDDEN LAYER INPUT
        hidden_input = np.dot(
            np.hstack([np.ones(shape=(X.shape[0], 1)), X]),
            self.w_input_
        )

        # CALCULATE HIDDEN LAYER OUTPUT
        hidden_output = np.tanh(hidden_input)
        hidden_output = np.hstack([np.ones(shape=(X.shape[0], 1)), hidden_output])

        # CALCULATE OUTPUT LAYER INPUT VALUES
        output_input = np.dot(
            hidden_output,
            self.w_output_
        )

        # OUTPUT LAYER ACTIVATEION FUNCTION IS LINEAR
        return output_input.reshape(-1, 1)
```

**Avaliação do `ELMClassifier`**

```{python}
model = ELMClassifier(neurons=10)
model.fit(data, target)
preds = model.predict(data)
print(classification_report(y_pred=preds > 0, y_true=target > 0))
```

**Teste com múltiplos números de neurônios na camada escondida**

```{python}
res = list()
for i in range(2, 100):
    model = ELMClassifier(neurons=i)
    model.fit(data, target)
    preds = model.predict(data)
    res.append({
        'neurons': i,
        'report': classification_report(y_pred=preds > 0, y_true=target > 0, output_dict=True)
    })

df_res = pd.json_normalize(res)
```

```{python}
plt.figure()
plt.scatter(df_res['neurons'], df_res['report.accuracy'])
plt.grid()
plt.show()
plt.close()
```

**Construção de grade de avaliação**

```{python}
steps = 100

grid_x0 = np.linspace(
    start=np.floor(np.min(data[:, 0])),
    stop=np.ceil(np.max(data[:, 0])),
    num=steps
)
grid_x1 = np.linspace(
    start=np.floor(np.min(data[:, 1])),
    stop=np.ceil(np.max(data[:, 1])),
    num=steps
)

grid = list()
for x0 in grid_x0:
    for x1 in grid_x1:
        grid.append([x0, x1])

grid = np.array(grid)
grid.shape
```

**Avaliação de modelo bem ajustado**

```{python}
model = ELMClassifier(neurons=20)
model.fit(data, target)
preds = model.predict(grid)
```

```{python}
plt.figure()
plt.imshow(
    (preds * -1 > 0).reshape(steps, steps),
    extent=(grid_x0[0], grid_x0[-1], grid_x1[0], grid_x1[-1]),
    alpha=.3
)
plt.scatter(data[:, 0], data[:, 1], c=target)
plt.show()
plt.close()
```

**Avaliação de modelo sobre-ajustado**

```{python}
model = ELMClassifier(neurons=200)
model.fit(data, target)
preds = model.predict(grid)
```

```{python}
plt.figure()
plt.imshow(
    (preds * -1 > 0).reshape(steps, steps),
    extent=(grid_x0[0], grid_x0[-1], grid_x1[0], grid_x1[-1]),
    alpha=.3
)
plt.scatter(data[:, 0], data[:, 1], c=target)
plt.show()
plt.close()
```
